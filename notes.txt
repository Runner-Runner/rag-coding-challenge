Questions:
[] Vectorize?
[] Which similarity measure to use? Library function? Part of vectorizing algorithm?
[] Which LLM to use?
	[x] Must be running locally on CPU? Is that even feasible with decent runtime?
		- Would be probably feasible, but I am also allowed to use a hosted model.
	- Must be able to know German. (or include translation step ...)
	- DiscoResearch/Llama3-DiscoLeo-Instruct-8B-v0.1-4bit-awq
		- awq not supported without GPU
- Embeddings:
	- FAISS PyPDF loader gives surprisingly poor performance out of the box. Doc splits seem logical. Did I choose a bad setting somewhere?
		- For questions containing an exact long ID string, I would it expect to return the right result at least in the top 4 every time.
		- https://www.youtube.com/watch?v=sKyvsdEv6rk
			- "In depth" explanation of FB AI Similarity Search (FAISS)
		- https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index
		- https://medium.com/loopio-tech/how-to-use-faiss-to-build-your-first-similarity-search-bf0f708aa772
		[] Case shouldn't be relevant, convert to lowercase if necessary?
	
Tasks:
[x] General Analysis PDFs:
	[x] First approach, just toss in file text as context and try it out?
	- Lists in file text are probably good enough as is
	- Contains key value pairs horizontally and vertically paired, might need explicit markings? (Not sure if typical in RAG preprocessing)
		- Especially vertically, because reading linewise means all keys are in one row and all values in another in that case, probably lost context?
	- Should blocks be connected to (document) title?
	- Must title be treated as special?
[] Convert PDFs to fitting format
	[] Provide script to replicate steps, in case the relevant corpus is extended or changed.
	- pypdf
		- Sufficient quality but dimensions SLOWER than other solutions like tika. BUT: can be run purely in python.
		- This is fast enough, but extraction makes mistakes; the order of text blocks seems randomly switched, and order seems highly important for relevancy evaluation.
		- langchain lib even has a PyPDFLoader/PyPDFDirectoryLoader to vectorize the context data directly
			- https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/
			[] Maybe class UnstructuredPDFLoader is helpful for the shifted title problem?
	- Tika
		- Using Java Runtime
		- Also orders some blocks wrongly ...
	[] Sanitization
		[] Maybe requires a manual fix of the block order?
			- Title (product name) is in the wrong order in every doc. Is the title handled in a special way? Higher "weight"?
		[x] Remove all _ (misread -)
		[] Lowercase everything? (Or can that be done somewhere more "high-level"?)
		[] Connect vertical table? (very specific fix ...)
[] Test result from scratch 
	- To ensure all necessary prerequisites are provided
	[x] Create script to create venv "venv_cc" in which the code should run
[] Test result on low-power laptop
[x] Write test suite
	[x] 4 base queries
	[] extensive (per doc)

Problems:
[x] LLMs are way too slow locally without GPU
	- llama_cpp_python is much faster even on CPU
	- Use quantized .gguf models
		- E.g. llama-2-13b-german-assistant-v4.Q2_K.gguf, Llama2 for German
		- https://huggingface.co/flozi00/Llama-2-13b-german-assistant-v4
		-> https://huggingface.co/TheBloke/Llama-2-13B-German-Assistant-v4-GGUF
[] "Llama.generate: 9 prefix-match hit, remaining 66 prompt tokens to eval" - is this indicating a wrong configuration?
[] Answer quality of llama-2-13b-german-assistant-v4.Q2_K.gguf might not be good enough
	[] Test other quantizations
		- llama-2-13b-german-assistant-v4.Q3_K_L.gguf
		- llama-2-13b-german-assistant-v4.Q8_0.gguf
	- Answer quality generally fluctuating strongly, maybe bad configuration.
	-> mistral gguf model is much better! Even though it doesn't answer in German in some cases, it has less failures and is much faster.
	- Q5 is recommended: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/blob/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf
[] Retrieval using FAISS doesn't work well enough
	- Doesn't return the relevant doc even for long unique identifiers (like 4008321299963)
		- Not suited for small datasets??
			- Using Flat for highest accuracy
	- Maybe PDF text/document must be preprocessed differently?
	- Embeddings for German language not good enough?
		- paraphrase-multilingual-MiniLM-L12-v2
		- FAISS vectorization also tested with english version of PDF documents, no significant improvement
	[] How do handle questions that need context from all docs?
		- Ex. last given question: "Gebe mir alle Leuchtmittel mit mindestens 1500W und einer Lebensdauer von mehr als 3000 Stunden?"
			- Even if splitting into small chunks like row-wise, need to give (document_count * 2) context chunks to LLM in this case
			- Other questions don't want this much context
			[] How to differentiate between these different context requirements? Check if relevancy score is evenly distributed? 
				-> but must also meet threshold, otherwise no document is relevant
	[] -> If I normalize L2 distances (to normalize relevance scores), results worsen.
	[x] Try to split into sentence-wise chunks
		- Improves retrieval results, seems to work well for non-running key-value-based text
		<-> Obviously increases problem with vertically-paired tables
		- Obviously splits up multi-row sentences, in this case those don't contain data that's as relevant
			-> Could increase including higher number of relevant chunks during retrieval
	[] Try (additional) naive TF-IDF similarity approach as baseline?

Links:
- https://learnbybuilding.ai/tutorials/rag-from-scratch
- (https://arxiv.org/abs/2005.11401)
- https://www.markhneedham.com/blog/2023/06/23/hugging-face-run-llm-model-locally-laptop/
	-> Very outdated, langchain was significantly overhauled since 2023
- https://huggingface.co/learn/cookbook/rag_zephyr_langchain
- Huggingface OS LLM leader board:
	- https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard
- https://medium.com/@penkow/how-to-run-llama-2-locally-on-cpu-docker-image-731eae6398d1
	- Using LLM running locally on CPU
	- llama-cpp-python??
	- pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu
	- German llama gguf version: https://huggingface.co/TheBloke/Llama-2-13B-German-Assistant-v4-GGUF
- https://discuss.huggingface.co/t/rag-embeddings-german-language/60840/3
	- German embeddings
- https://docs.llamaindex.ai/en/stable/examples/embeddings/huggingface/
	- Completely different "holistic" alternative, probably no time to test it out